#!/usr/bin/env python3
"""
Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è Sofia Bot
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    # –¢–æ–ª—å–∫–æ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ (–±—ã—Å—Ç—Ä–æ, –±–µ—Å–ø–ª–∞—Ç–Ω–æ)
    python batch_process.py ./dialogs/ --output dataset.jsonl
    
    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ + LLM GPT-5.2 (—Ç–æ—á–Ω–æ)
    python batch_process.py ./dialogs/ --llm --provider openai --api-key sk-xxx --output dataset.jsonl
"""

import argparse
import json
import time
from pathlib import Path
from dialog_processor import process_file, DialogScore, llm_score_dialog, parse_dialog, decode_rtf, dialog_to_jsonl
from typing import List, Dict

def print_report(results: List[Dict], llm_used: bool = False):
    """–ü–µ—á–∞—Ç–∞–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π –æ—Ç—á—ë—Ç"""
    
    print("\n" + "="*70)
    print("üìä –ê–ù–ê–õ–ò–¢–ò–ö–ê –î–ò–ê–õ–û–ì–û–í OAZIS ESTATE")
    print("="*70)
    
    total = len(results)
    llm_evaluated = sum(1 for r in results if r.get('llm_evaluated', False))
    goals_achieved = sum(1 for r in results if r.get('goal_achieved', False))
    
    # –ü–æ–¥—Å—á—ë—Ç –ø–æ –Ω–∞–≤—ã–∫–∞–º
    good_for_qualification = sum(1 for r in results if r.get('good_for_qualification', False))
    good_for_closing = sum(1 for r in results if r.get('good_for_closing', False))
    good_for_objections = sum(1 for r in results if r.get('good_for_objections', False))
    good_for_anything = sum(1 for r in results if r.get('good_for_training', False))
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    by_result = {}
    for r in results:
        by_result[r['result']] = by_result.get(r['result'], 0) + 1
    
    print(f"\nüìÅ –í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤: {total}")
    if llm_used:
        print(f"ü§ñ LLM-–æ—Ü–µ–Ω–µ–Ω–æ: {llm_evaluated}")
    print(f"üéØ –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ (—Å–æ–∑–≤–æ–Ω): {goals_achieved} ({goals_achieved/total*100:.1f}%)")
    
    print(f"\n{'='*70}")
    print("üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–ò–ê–õ–û–ì–û–í")
    print("="*70)
    result_labels = {
        '—Å–æ–∑–≤–æ–Ω': 'üéØ –°–æ–∑–≤–æ–Ω –Ω–∞–∑–Ω–∞—á–µ–Ω',
        '–¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç—å': 'ü§ù –ï—Å—Ç—å –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç—å',
        '–≤_–ø—Ä–æ—Ü–µ—Å—Å–µ': '‚è≥ –í –ø—Ä–æ—Ü–µ—Å—Å–µ',
        '–æ—Ç–∫–∞–∑': '‚ùå –û—Ç–∫–∞–∑',
        '–∏–≥–Ω–æ—Ä': 'üëª –ò–≥–Ω–æ—Ä/–∑–∞–≥–ª–æ—Ö',
        '–Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω': '‚ùì –ù–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω'
    }
    for res, count in sorted(by_result.items(), key=lambda x: -x[1]):
        pct = count/total*100
        bar = "‚ñà" * int(pct/3)
        label = result_labels.get(res, res)
        print(f"   {label:25} {count:3} ({pct:5.1f}%) {bar}")
    
    # === –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ù–ê–í–´–ö–ê–ú ===
    if llm_used:
        print(f"\n{'='*70}")
        print("üéì –ü–û–î–•–û–î–Ø–¢ –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø (–ø–æ –Ω–∞–≤—ã–∫–∞–º)")
        print("="*70)
        
        skills_data = [
            ("üìã –ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è", good_for_qualification, "good_for_qualification"),
            ("üìû –ó–∞–∫—Ä—ã—Ç–∏–µ –Ω–∞ —Å–æ–∑–≤–æ–Ω", good_for_closing, "good_for_closing"),
            ("üõ°Ô∏è –û—Ç—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π", good_for_objections, "good_for_objections"),
        ]
        
        for label, count, _ in skills_data:
            pct = count/llm_evaluated*100 if llm_evaluated else 0
            bar = "‚ñà" * int(pct/3)
            print(f"   {label:30} {count:3} ({pct:5.1f}%) {bar}")
        
        print(f"\n   {'‚îÄ'*50}")
        print(f"   üì¶ –í—Å–µ–≥–æ –ø–æ–ª–µ–∑–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤:    {good_for_anything:3} ({good_for_anything/llm_evaluated*100:.1f}%)")
        
        # –°—Ä–µ–¥–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ –Ω–∞–≤—ã–∫–∞–º
        qual_scores = [r['qualification'].get('score', 0) for r in results if r.get('qualification')]
        close_scores = [r['closing'].get('score', 0) for r in results if r.get('closing')]
        obj_scores = [r['objection_handling'].get('score', 0) for r in results 
                      if r.get('objection_handling') and r['objection_handling'].get('score') is not None]
        
        print(f"\n{'='*70}")
        print("üìä –°–†–ï–î–ù–ò–ï –û–¶–ï–ù–ö–ò –ü–û –ù–ê–í–´–ö–ê–ú")
        print("="*70)
        if qual_scores:
            avg = sum(qual_scores)/len(qual_scores)
            bar = "‚ñà" * int(avg)
            print(f"   üìã –ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è:           {avg:.1f}/10 {bar}")
        if close_scores:
            avg = sum(close_scores)/len(close_scores)
            bar = "‚ñà" * int(avg)
            print(f"   üìû –ó–∞–∫—Ä—ã—Ç–∏–µ –Ω–∞ —Å–æ–∑–≤–æ–Ω:     {avg:.1f}/10 {bar}")
        if obj_scores:
            avg = sum(obj_scores)/len(obj_scores)
            bar = "‚ñà" * int(avg)
            print(f"   üõ°Ô∏è –û—Ç—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π:   {avg:.1f}/10 {bar}")
    
    # === –î–ï–¢–ê–õ–¨–ù–´–ô –†–ê–ó–ë–û–† –ü–û –ö–ê–ñ–î–û–ú–£ –î–ò–ê–õ–û–ì–£ ===
    print(f"\n{'='*70}")
    print("üìã –î–ï–¢–ê–õ–¨–ù–´–ô –†–ê–ó–ë–û–† –î–ò–ê–õ–û–ì–û–í")
    print("="*70)
    
    for r in results:
        if not r.get('llm_evaluated'):
            continue
        
        print(f"\n{'‚îÄ'*70}")
        print(f"üìÅ {r['file']}")
        print(f"üìä –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {r['score']}/10 | –†–µ–∑—É–ª—å—Ç–∞—Ç: {r['result']} | –¶–µ–ª—å: {'üéØ –î–∞' if r.get('goal_achieved') else '‚ùå –ù–µ—Ç'}")
        
        # –ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è
        qual = r.get('qualification', {})
        if qual:
            status = "‚úÖ" if qual.get('good_for_training') else "‚ùå"
            score = qual.get('score', '?')
            learned = []
            if qual.get('learned_goal'): learned.append('—Ü–µ–ª—å')
            if qual.get('learned_location'): learned.append('–ª–æ–∫–∞—Ü–∏—è')
            if qual.get('learned_budget'): learned.append('–±—é–¥–∂–µ—Ç')
            learned_str = ', '.join(learned) if learned else '–Ω–∏—á–µ–≥–æ'
            print(f"\n   üìã –ö–í–ê–õ–ò–§–ò–ö–ê–¶–ò–Ø: {score}/10 {status}")
            print(f"      –£–∑–Ω–∞–ª: {learned_str}")
            if qual.get('verdict'):
                print(f"      ‚Üí {qual['verdict']}")
        
        # –ó–∞–∫—Ä—ã—Ç–∏–µ
        close = r.get('closing', {})
        if close:
            status = "‚úÖ" if close.get('good_for_training') else "‚ùå"
            score = close.get('score', '?')
            actions = []
            if close.get('call_offered'): actions.append('–ø—Ä–µ–¥–ª–æ–∂–∏–ª —Å–æ–∑–≤–æ–Ω')
            if close.get('call_scheduled'): actions.append('–Ω–∞–∑–Ω–∞—á–∏–ª')
            if close.get('value_explained'): actions.append('–æ–±—ä—è—Å–Ω–∏–ª —Ü–µ–Ω–Ω–æ—Å—Ç—å')
            actions_str = ', '.join(actions) if actions else '–Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞–ª'
            print(f"\n   üìû –ó–ê–ö–†–´–¢–ò–ï –ù–ê –°–û–ó–í–û–ù: {score}/10 {status}")
            print(f"      –î–µ–π—Å—Ç–≤–∏—è: {actions_str}")
            if close.get('verdict'):
                print(f"      ‚Üí {close['verdict']}")
        
        # –í–æ–∑—Ä–∞–∂–µ–Ω–∏—è
        obj = r.get('objection_handling', {})
        if obj and obj.get('had_objections'):
            status = "‚úÖ" if obj.get('good_for_training') else "‚ùå"
            score = obj.get('score', '?')
            handled = "–¥–∞" if obj.get('handled_well') else "–Ω–µ—Ç"
            print(f"\n   üõ°Ô∏è –í–û–ó–†–ê–ñ–ï–ù–ò–Ø: {score}/10 {status}")
            print(f"      –ë—ã–ª–∏ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è: –¥–∞, –æ—Ç—Ä–∞–±–æ—Ç–∞–ª —Ö–æ—Ä–æ—à–æ: {handled}")
            if obj.get('verdict'):
                print(f"      ‚Üí {obj['verdict']}")
        
        # –û—à–∏–±–∫–∏
        if r.get('mistakes'):
            print(f"\n   ‚ùå –û—à–∏–±–∫–∏:")
            for m in r['mistakes'][:3]:
                print(f"      ‚Ä¢ {m[:75]}")
        
        # –†–µ–∑—é–º–µ
        if r.get('summary'):
            print(f"\n   üí¨ {r['summary']}")
    
    # === –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–ó–Æ–ú–ï –ü–û –ù–ê–í–´–ö–ê–ú ===
    print(f"\n{'='*70}")
    print("üìù –ò–¢–û–ì–û–í–û–ï –†–ï–ó–Æ–ú–ï: –ß–¢–û –ú–û–ñ–ù–û –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨")
    print("="*70)
    
    # –î–ª—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏
    qual_dialogs = [r for r in results if r.get('good_for_qualification')]
    print(f"\nüìã –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø –ö–í–ê–õ–ò–§–ò–ö–ê–¶–ò–ò: {len(qual_dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤")
    for r in qual_dialogs[:5]:
        verdict = r.get('qualification', {}).get('verdict', '')[:60]
        print(f"   ‚úÖ {r['file'][:35]}")
        if verdict:
            print(f"      {verdict}")
    if len(qual_dialogs) > 5:
        print(f"   ... –∏ –µ—â—ë {len(qual_dialogs) - 5}")
    
    # –î–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è
    close_dialogs = [r for r in results if r.get('good_for_closing')]
    print(f"\nüìû –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø –ó–ê–ö–†–´–¢–ò–Ø –ù–ê –°–û–ó–í–û–ù: {len(close_dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤")
    for r in close_dialogs[:5]:
        verdict = r.get('closing', {}).get('verdict', '')[:60]
        print(f"   ‚úÖ {r['file'][:35]}")
        if verdict:
            print(f"      {verdict}")
    if len(close_dialogs) > 5:
        print(f"   ... –∏ –µ—â—ë {len(close_dialogs) - 5}")
    
    # –î–ª—è –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π
    obj_dialogs = [r for r in results if r.get('good_for_objections')]
    print(f"\nüõ°Ô∏è –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø –í–û–ó–†–ê–ñ–ï–ù–ò–ô: {len(obj_dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤")
    for r in obj_dialogs[:5]:
        verdict = r.get('objection_handling', {}).get('verdict', '')[:60]
        print(f"   ‚úÖ {r['file'][:35]}")
        if verdict:
            print(f"      {verdict}")
    if len(obj_dialogs) > 5:
        print(f"   ... –∏ –µ—â—ë {len(obj_dialogs) - 5}")
    
    # –ù–µ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–∏ –¥–ª—è —á–µ–≥–æ
    useless = [r for r in results if r.get('llm_evaluated') and not r.get('good_for_training')]
    print(f"\n‚ùå –ù–ï –ü–û–î–•–û–î–Ø–¢ –ù–ò –î–õ–Ø –ß–ï–ì–û: {len(useless)} –¥–∏–∞–ª–æ–≥–æ–≤")

def save_jsonl(results: List[Dict], output_path: str, min_score: int = 5):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ö–æ—Ä–æ—à–∏–µ –¥–∏–∞–ª–æ–≥–∏ –≤ JSONL"""
    
    good_dialogs = [r for r in results if r['score'] >= min_score and r['jsonl']]
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for r in good_dialogs:
            f.write(json.dumps(r['jsonl'], ensure_ascii=False) + '\n')
    
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(good_dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤ –≤ {output_path}")
    print(f"   (score >= {min_score})")

def main():
    parser = argparse.ArgumentParser(description='Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤')
    parser.add_argument('input', help='–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∏–∞–ª–æ–≥–∞–º–∏ –∏–ª–∏ —Ñ–∞–π–ª—É')
    parser.add_argument('--min-score', type=int, default=5, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –¥–∞—Ç–∞—Å–µ—Ç')
    parser.add_argument('--output', '-o', default='dataset.jsonl', help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSONL')
    parser.add_argument('--managers', nargs='+', default=['–°–æ—Ñ–∏—è', '–°–µ—Ä–≥–µ–π', '–í–∏–∫—Ç–æ—Ä–∏—è', '–û–∫—Å–∞–Ω–∞', '–Æ–ª–∏—è', '–ò–≥–æ—Ä—å'],
                       help='–ò–º–µ–Ω–∞ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤')
    
    # LLM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--llm', action='store_true', help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤')
    parser.add_argument('--provider', choices=['openai', 'anthropic'], default='openai', help='LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä')
    parser.add_argument('--api-key', help='API –∫–ª—é—á (–∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY / ANTHROPIC_API_KEY)')
    parser.add_argument('--model', help='–ú–æ–¥–µ–ª—å (default: gpt-5.2 –∏–ª–∏ claude-3-haiku)')
    parser.add_argument('--llm-threshold', type=int, default=4, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π score –¥–ª—è LLM-–æ—Ü–µ–Ω–∫–∏')
    
    args = parser.parse_args()
    
    # –ü–æ–ª—É—á–∞–µ–º API key
    api_key = args.api_key
    if args.llm and not api_key:
        import os
        if args.provider == 'openai':
            api_key = os.environ.get('OPENAI_API_KEY')
        else:
            api_key = os.environ.get('ANTHROPIC_API_KEY')
        
        if not api_key:
            print(f"‚ùå –ù—É–∂–µ–Ω API –∫–ª—é—á! –£–∫–∞–∂–∏—Ç–µ --api-key –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è")
            return
    
    input_path = Path(args.input)
    results = []
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª—ã
    if input_path.is_file():
        files = [input_path]
    else:
        files = list(input_path.glob('*.txt')) + list(input_path.glob('*.rtf'))
    
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
    if args.llm:
        print(f"ü§ñ LLM-–æ—Ü–µ–Ω–∫–∞ –≤–∫–ª—é—á–µ–Ω–∞: {args.provider} (threshold >= {args.llm_threshold})")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
    llm_count = 0
    for i, filepath in enumerate(files, 1):
        try:
            messages, score, jsonl_data = process_file(str(filepath), args.managers)
            
            result_entry = {
                'file': filepath.name,
                'messages_count': len(messages),
                'heuristic_score': score.total,
                'score': score.total,
                'result': score.result,
                'goal_achieved': False,
                'qualification': {},
                'closing': {},
                'objection_handling': {},
                'communication': {},
                'good_for_qualification': False,
                'good_for_closing': False,
                'good_for_objections': False,
                'good_for_training': False,
                'strengths': [],
                'mistakes': [],
                'summary': '',
                'llm_evaluated': False,
                'jsonl': None
            }
            
            # LLM-–æ—Ü–µ–Ω–∫–∞ –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            if args.llm and score.total >= args.llm_threshold:
                try:
                    print(f"   ü§ñ LLM-–æ—Ü–µ–Ω–∫–∞: {filepath.name[:30]}...")
                    llm_result = llm_score_dialog(messages, api_key, args.provider, args.model)
                    
                    result_entry['score'] = llm_result.get('overall_score', score.total)
                    result_entry['result'] = llm_result.get('result', score.result)
                    result_entry['goal_achieved'] = llm_result.get('goal_achieved', False)
                    
                    # –û—Ü–µ–Ω–∫–∏ –ø–æ –Ω–∞–≤—ã–∫–∞–º
                    result_entry['qualification'] = llm_result.get('qualification', {})
                    result_entry['closing'] = llm_result.get('closing', {})
                    result_entry['objection_handling'] = llm_result.get('objection_handling', {})
                    result_entry['communication'] = llm_result.get('communication', {})
                    
                    result_entry['strengths'] = llm_result.get('strengths', [])
                    result_entry['mistakes'] = llm_result.get('mistakes', [])
                    result_entry['summary'] = llm_result.get('summary', '')
                    
                    # –§–ª–∞–≥–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ –Ω–∞–≤—ã–∫–∞–º
                    result_entry['good_for_qualification'] = result_entry['qualification'].get('good_for_training', False)
                    result_entry['good_for_closing'] = result_entry['closing'].get('good_for_training', False)
                    result_entry['good_for_objections'] = result_entry['objection_handling'].get('good_for_training', False)
                    
                    # –û–±—â–∏–π —Ñ–ª–∞–≥ ‚Äî –ø–æ–¥—Ö–æ–¥–∏—Ç —Ö–æ—Ç—è –±—ã –¥–ª—è —á–µ–≥–æ-—Ç–æ
                    result_entry['good_for_training'] = (
                        result_entry['good_for_qualification'] or 
                        result_entry['good_for_closing'] or 
                        result_entry['good_for_objections']
                    )
                    
                    result_entry['llm_evaluated'] = True
                    llm_count += 1
                    
                    # === –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–ê –°–†–ê–ó–£ ===
                    goal_icon = "üéØ" if result_entry['goal_achieved'] else "‚ùå"
                    qual = result_entry['qualification']
                    close = result_entry['closing']
                    obj = result_entry['objection_handling']
                    
                    qual_score = qual.get('score', '-')
                    qual_ok = "‚úÖ" if result_entry['good_for_qualification'] else "‚ùå"
                    close_score = close.get('score', '-')
                    close_ok = "‚úÖ" if result_entry['good_for_closing'] else "‚ùå"
                    obj_score = obj.get('score', '-') if obj.get('had_objections') else "‚Äî"
                    obj_ok = "‚úÖ" if result_entry['good_for_objections'] else ("‚ùå" if obj.get('had_objections') else "‚Äî")
                    
                    print(f"      üìä {result_entry['score']}/10 | {result_entry['result']} | –¶–µ–ª—å: {goal_icon}")
                    print(f"      üìã –ö–≤–∞–ª–∏—Ñ: {qual_score}/10 {qual_ok} | üìû –ó–∞–∫—Ä—ã—Ç–∏–µ: {close_score}/10 {close_ok} | üõ°Ô∏è –í–æ–∑—Ä–∞–∂: {obj_score} {obj_ok}")
                    
                    if result_entry['good_for_training']:
                        skills = []
                        if result_entry['good_for_qualification']: skills.append("–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è")
                        if result_entry['good_for_closing']: skills.append("–∑–∞–∫—Ä—ã—Ç–∏–µ")
                        if result_entry['good_for_objections']: skills.append("–≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è")
                        print(f"      ‚úÖ –ü–û–î–•–û–î–ò–¢ –¥–ª—è: {', '.join(skills)}")
                    else:
                        print(f"      ‚ùå –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                    
                    if result_entry['summary']:
                        print(f"      üí¨ {result_entry['summary'][:80]}...")
                    print()
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å rate limit
                    time.sleep(0.5)
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è LLM –æ—à–∏–±–∫–∞: {e}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º jsonl –µ—Å–ª–∏ score –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π
            final_score = result_entry['score']
            if final_score >= args.min_score:
                # –î–ª—è LLM-–æ—Ü–µ–Ω—ë–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä—è–µ–º good_for_training
                if result_entry['llm_evaluated']:
                    if result_entry.get('good_for_training', False):
                        result_entry['jsonl'] = jsonl_data
                else:
                    result_entry['jsonl'] = jsonl_data
            
            results.append(result_entry)
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å
            if i % 10 == 0:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(files)}")
                
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ {filepath.name}: {e}")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score
    results = sorted(results, key=lambda x: x['score'], reverse=True)
    
    # –û—Ç—á—ë—Ç
    print_report(results, llm_used=args.llm)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    save_jsonl(results, args.output, args.min_score)
    
    # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç
    report_path = args.output.replace('.jsonl', '_report.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        # –£–±–∏—Ä–∞–µ–º jsonl –∏–∑ –æ—Ç—á—ë—Ç–∞ (—Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π)
        report_results = [{k: v for k, v in r.items() if k != 'jsonl'} for r in results]
        json.dump(report_results, f, ensure_ascii=False, indent=2)
    print(f"üìã –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {report_path}")
    
    if args.llm:
        print(f"\nü§ñ LLM-–æ—Ü–µ–Ω–µ–Ω–æ –¥–∏–∞–ª–æ–≥–æ–≤: {llm_count}")

if __name__ == '__main__':
    main()
